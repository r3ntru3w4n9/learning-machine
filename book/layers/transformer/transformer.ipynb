{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention is all you need\n",
    "\n",
    "The transformer model comes out of the paper _attention is all you need_. The paper shows how powerful pure attention mechanisms can be. They introduced a new kind of attention mechanism called self-attention, which we'll discuss later.\n",
    "\n",
    "The significance about self-attention is that with only attention mechanism, the model achieves state-of-the-art performance on many datasets, in a field previously dominated by RNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the transformer work?\n",
    "\n",
    "The transformer architecture is based on a Seq2Seq model. Traditionally, a seq2seq model is basically an encoder and a decoder, like auto-encoders, but both encoder and decoder are RNNs. The encoder first process through the input, then feeds the encoder's RNN state or output to the decoder to decode the full sentence. The idea is that the encoder should be able to encode the input into some kind of representation that contains the meaning of the sentence, and the decoder should be able to understand that representation.\n",
    "\n",
    "In the case of transformers, because it's not an RNN, so instead of RNN state, the attention produced by the encoder is used and sent to the decoder. Decoder uses that global information to produce the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer encoder\n",
    "\n",
    "The encoding component is a stack of smaller encoders. An encoder does the following thing\n",
    "\n",
    "1. Calculate self-attention score for the input $ I $.\n",
    "2. Weigh the input by self-attention scores $ S(I) $.\n",
    "3. Pass it through an add-and-normalize layer $ O = I + S(I) $.\n",
    "4. Feed the processed data through a linear layer $ F = f(O) $.\n",
    "5. Perform activation on the linear layer's output $ F' = \\sigma(F) $.\n",
    "6. Multiply the mutated output with the output itself $ F'' = F'F $.\n",
    "7. Pass it through an add-and-normalize layer $ F'' + O $.\n",
    "\n",
    "An add-and-normalize layer performs a residual add, adding the input and the processed input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer decoder\n",
    "\n",
    "The decoding component look a lot like the encoding component, it's also a stack of decoders. Decoders are basically encoders, but they take the attention provided by encoders, and perform step $ 3 $ twice, the first time adds it by the decoder generated attention, the second time adds by the encoder generated attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
